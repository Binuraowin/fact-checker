# -*- coding: utf-8 -*-
"""lanhchain-basic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13vpu3FGurr9rlRe8ALhREcBptsW6QdeV
"""

import nltk
import os

# Initialize NLTK and download required resources
try:
    nltk.download('punkt', quiet=True)
except Exception as e:
    print(f"Error downloading NLTK resources: {str(e)}")

from typing import Dict, List, Optional, Tuple
import requests
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi
import re
from transformers import pipeline
from nltk.tokenize import sent_tokenize
import pandas as pd
import spacy

class ContentExtractor:
    def __init__(self):
        # Initialize spacy with fallback
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("Downloading spacy model...")
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")

    def extract_from_url(self, url: str) -> Tuple[str, str]:
        """Extract content based on URL type (news article or YouTube)."""
        if "youtube.com" in url or "youtu.be" in url:
            return self._extract_youtube_content(url)
        else:
            return self._extract_article_content(url)

    def _extract_youtube_content(self, url: str) -> Tuple[str, str]:
        """Extract transcript and metadata from YouTube videos."""
        video_id = self._get_youtube_id(url)
        try:
            transcript = YouTubeTranscriptApi.get_transcript(video_id)
            full_text = ' '.join([entry['text'] for entry in transcript])
            return "youtube", full_text
        except Exception as e:
            raise Exception(f"Failed to extract YouTube content: {str(e)}")

    def _extract_article_content(self, url: str) -> Tuple[str, str]:
        """Extract content from news articles."""
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # Remove unwanted elements
            for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
                tag.decompose()

            # Extract main content
            article_text = ' '.join([p.text for p in soup.find_all('p')])
            return "article", article_text
        except Exception as e:
            raise Exception(f"Failed to extract article content: {str(e)}")

    def _get_youtube_id(self, url: str) -> str:
        """Extract YouTube video ID from URL."""
        pattern = r'(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})'
        match = re.search(pattern, url)
        if match:
            return match.group(1)
        raise ValueError("Invalid YouTube URL")

class FactChecker:
    def __init__(self):
        # Initialize with error handling for transformers
        try:
            self.zero_shot_classifier = pipeline("zero-shot-classification")
            self.qa_pipeline = pipeline("question-answering")
        except Exception as e:
            print(f"Error initializing transformers: {str(e)}")
            raise

        self.fact_database = self._load_fact_database()

    def _load_fact_database(self) -> pd.DataFrame:
        """Load verified facts database (placeholder - would be replaced with actual database)."""
        return pd.DataFrame({
            'topic': ['COVID-19', 'Climate Change'],
            'fact': ['COVID-19 vaccines are safe and effective', 'Global temperatures are rising'],
            'source': ['WHO', 'IPCC'],
            'url': ['https://www.who.int', 'https://www.ipcc.ch']
        })

    def check_facts(self, content: str) -> List[Dict]:
        """Check content for factual accuracy."""
        try:
            # Break content into sentences
            sentences = sent_tokenize(content)
        except Exception as e:
            print(f"Error in sentence tokenization: {str(e)}")
            sentences = [content]  # Fallback to treating entire content as one sentence

        fact_checks = []

        for sentence in sentences:
            # Classify sentence topics
            try:
                topics = self._classify_topics(sentence)
            except Exception as e:
                print(f"Error in topic classification: {str(e)}")
                continue

            # Check each topic against fact database
            for topic in topics:
                relevant_facts = self._get_relevant_facts(topic)

                # Compare sentence against facts
                try:
                    contradiction = self._check_contradiction(sentence, relevant_facts)
                    if contradiction:
                        fact_checks.append({
                            'claim': sentence,
                            'correction': contradiction['fact'],
                            'source': contradiction['source'],
                            'source_url': contradiction['url']
                        })
                except Exception as e:
                    print(f"Error checking contradiction: {str(e)}")
                    continue

        return fact_checks

    def _classify_topics(self, sentence: str) -> List[str]:
        """Classify the topics present in a sentence."""
        candidate_topics = self.fact_database['topic'].unique().tolist()
        result = self.zero_shot_classifier(sentence, candidate_topics)
        return [topic for topic, score in zip(result['labels'], result['scores']) if score > 0.5]

    def _get_relevant_facts(self, topic: str) -> pd.DataFrame:
        """Get relevant facts for a given topic."""
        return self.fact_database[self.fact_database['topic'] == topic]

    def _check_contradiction(self, claim: str, facts: pd.DataFrame) -> Optional[Dict]:
        """Check if claim contradicts any known facts."""
        for _, fact in facts.iterrows():
            # Use question-answering to check contradiction
            question = f"Does this claim contradict the fact that {fact['fact']}?"
            result = self.qa_pipeline(question=question, context=claim)

            if result['score'] > 0.8 and 'yes' in result['answer'].lower():
                return fact.to_dict()
        return None

class FactCheckingApp:
    def __init__(self):
        # Initialize components with proper error handling
        try:
            self.content_extractor = ContentExtractor()
            self.fact_checker = FactChecker()
        except Exception as e:
            raise Exception(f"Failed to initialize FactCheckingApp: {str(e)}")

    def process_url(self, url: str) -> Dict:
        """Process a URL and return fact-checking results."""
        try:
            # Extract content
            content_type, content = self.content_extractor.extract_from_url(url)

            # Check facts
            fact_check_results = self.fact_checker.check_facts(content)

            return {
                'url': url,
                'content_type': content_type,
                'fact_checks': fact_check_results,
                'status': 'success'
            }
        except Exception as e:
            return {
                'url': url,
                'status': 'error',
                'error': str(e)
            }

def setup_environment():
    """Setup function to ensure all required resources are downloaded."""
    try:
        # Download NLTK data
        nltk.download('punkt')

        # Check spaCy model
        try:
            spacy.load('en_core_web_sm')
        except OSError:
            os.system('python -m spacy download en_core_web_sm')

        print("Environment setup completed successfully.")
        return True
    except Exception as e:
        print(f"Error setting up environment: {str(e)}")
        return False

# Example usage
if __name__ == "__main__":
    # Setup environment first
    if setup_environment():
        try:
            app = FactCheckingApp()

            # Example URL
            url = "https://www.youtube.com/watch?v=yRT1Q3xwkKg&ab_channel=AdaDerana"
            results = app.process_url(url)

            # Print results
            if results['status'] == 'success':
                print(f"Content type: {results['content_type']}")
                if results['fact_checks']:
                    print("\nFact check results:")
                    for check in results['fact_checks']:
                        print(f"\nClaim: {check['claim']}")
                        print(f"Correction: {check['correction']}")
                        print(f"Source: {check['source']}")
                        print(f"Source URL: {check['source_url']}")
                else:
                    print("\nNo factual inaccuracies found.")
            else:
                print(f"Error: {results['error']}")
        except Exception as e:
            print(f"Application error: {str(e)}")
    else:
        print("Failed to set up environment. Please check the error messages above.")

!pip install beautifulsoup4 youtube_transcript_api transformers nltk spacy pandas requests
!python -m spacy download en_core_web_sm

import nltk
import os
from typing import Dict, List, Optional, Tuple
import requests
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi
import re
from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer
import pandas as pd
import spacy
from googletrans import Translator
import json
from datetime import datetime
import unicodedata

# Base ContentExtractor class
class ContentExtractor:
    def __init__(self):
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("Downloading spacy model...")
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")

    def extract_from_url(self, url: str) -> Tuple[str, str]:
        if "youtube.com" in url or "youtu.be" in url:
            return self._extract_youtube_content(url)
        else:
            return self._extract_article_content(url)

    def _extract_youtube_content(self, url: str) -> Tuple[str, str]:
        video_id = self._get_youtube_id(url)
        try:
            transcript = YouTubeTranscriptApi.get_transcript(video_id)
            full_text = ' '.join([entry['text'] for entry in transcript])
            return "youtube", full_text
        except Exception as e:
            raise Exception(f"Failed to extract YouTube content: {str(e)}")

    def _extract_article_content(self, url: str) -> Tuple[str, str]:
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
                tag.decompose()
            article_text = ' '.join([p.text for p in soup.find_all('p')])
            return "article", article_text
        except Exception as e:
            raise Exception(f"Failed to extract article content: {str(e)}")

    def _get_youtube_id(self, url: str) -> str:
        pattern = r'(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})'
        match = re.search(pattern, url)
        if match:
            return match.group(1)
        raise ValueError("Invalid YouTube URL")

# Base FactChecker class
class FactChecker:
    def __init__(self):
        self.zero_shot_classifier = pipeline("zero-shot-classification")
        self.qa_pipeline = pipeline("question-answering")
        self.fact_database = self._load_fact_database()

    def _load_fact_database(self) -> pd.DataFrame:
        return pd.DataFrame({
            'topic': ['COVID-19', 'Climate Change'],
            'fact': ['COVID-19 vaccines are safe and effective', 'Global temperatures are rising'],
            'source': ['WHO', 'IPCC'],
            'url': ['https://www.who.int', 'https://www.ipcc.ch']
        })

    def check_facts(self, content: str) -> List[Dict]:
        try:
            sentences = sent_tokenize(content)
        except Exception as e:
            print(f"Error in sentence tokenization: {str(e)}")
            sentences = [content]

        fact_checks = []
        for sentence in sentences:
            try:
                topics = self._classify_topics(sentence)
                for topic in topics:
                    relevant_facts = self._get_relevant_facts(topic)
                    contradiction = self._check_contradiction(sentence, relevant_facts)
                    if contradiction:
                        fact_checks.append({
                            'claim': sentence,
                            'correction': contradiction['fact'],
                            'source': contradiction['source'],
                            'source_url': contradiction['url']
                        })
            except Exception as e:
                print(f"Error checking sentence: {str(e)}")
                continue
        return fact_checks

    def _classify_topics(self, sentence: str) -> List[str]:
        candidate_topics = self.fact_database['topic'].unique().tolist()
        result = self.zero_shot_classifier(sentence, candidate_topics)
        return [topic for topic, score in zip(result['labels'], result['scores']) if score > 0.5]

    def _get_relevant_facts(self, topic: str) -> pd.DataFrame:
        return self.fact_database[self.fact_database['topic'] == topic]

    def _check_contradiction(self, claim: str, facts: pd.DataFrame) -> Optional[Dict]:
        for _, fact in facts.iterrows():
            question = f"Does this claim contradict the fact that {fact['fact']}?"
            result = self.qa_pipeline(question=question, context=claim)
            if result['score'] > 0.8 and 'yes' in result['answer'].lower():
                return fact.to_dict()
        return None

class SinhalaProcessor:
    def __init__(self):
        self.translator = Translator()
        self.translation_model = AutoModelForSeq2SeqLM.from_pretrained("Aloka/mbart50-ft-si-en")
        self.translation_tokenizer = AutoTokenizer.from_pretrained("Aloka/mbart50-ft-si-en")
        self.sinhala_stop_words = self._load_sinhala_stop_words()

    def _load_sinhala_stop_words(self) -> List[str]:
        try:
            with open('sinhala_stop_words.txt', 'r', encoding='utf-8') as f:
                return [line.strip() for line in f.readlines()]
        except FileNotFoundError:
            return ['සහ', 'හා', 'වැනි', 'සඳහා', 'පිළිබඳ', 'විසින්']

    def preprocess_sinhala_text(self, text: str) -> str:
        text = unicodedata.normalize('NFKC', text)
        words = text.split()
        words = [w for w in words if w not in self.sinhala_stop_words]
        return ' '.join(words)

    def translate_to_english(self, text: str) -> str:
        try:
            inputs = self.translation_tokenizer(text, return_tensors="pt", padding=True)
            outputs = self.translation_model.generate(**inputs)
            translation = self.translation_tokenizer.decode(outputs[0], skip_special_tokens=True)
            return translation
        except Exception as e:
            print(f"Specialized translation failed, falling back to Google Translate: {str(e)}")
            try:
                return self.translator.translate(text, src='si', dest='en').text
            except Exception as e:
                print(f"Translation failed: {str(e)}")
                return text

class SriLankanNewsExtractor:
    def __init__(self):
        self.sinhala_processor = SinhalaProcessor()
        self.known_sources = {
            'www.hirunews.lk': {
                'content_selector': 'div.news-content',
                'date_selector': 'span.date',
                'language': 'si'
            },
            'www.ada.lk': {
                'content_selector': 'div.article-content',
                'date_selector': 'span.published-date',
                'language': 'si'
            }
        }

    def extract_content(self, url: str) -> Dict:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')

            domain = re.search(r'(?:https?://)?(?:www\.)?([^/]+)', url).group(1)

            if domain in self.known_sources:
                rules = self.known_sources[domain]
                content = soup.select_one(rules['content_selector'])
                date = soup.select_one(rules['date_selector'])

                if content:
                    text = content.get_text(strip=True)
                    if rules['language'] == 'si':
                        text = self.sinhala_processor.preprocess_sinhala_text(text)
                        english_text = self.sinhala_processor.translate_to_english(text)
                        return {
                            'original_text': text,
                            'english_text': english_text,
                            'date': date.get_text(strip=True) if date else None,
                            'language': 'si'
                        }
                    return {
                        'original_text': text,
                        'english_text': text,
                        'date': date.get_text(strip=True) if date else None,
                        'language': 'en'
                    }

            content = ' '.join([p.get_text(strip=True) for p in soup.find_all('p')])
            return {
                'original_text': content,
                'english_text': self.sinhala_processor.translate_to_english(content),
                'date': None,
                'language': 'unknown'
            }

        except Exception as e:
            raise Exception(f"Failed to extract content: {str(e)}")

class EnhancedFactChecker(FactChecker):
    def __init__(self):
        super().__init__()
        self.sinhala_processor = SinhalaProcessor()
        self.sri_lankan_facts = self._load_sri_lankan_facts()

    def _load_sri_lankan_facts(self) -> pd.DataFrame:
        return pd.DataFrame({
            'topic': ['Sri Lankan Politics', 'Sri Lankan Economy'],
            'fact_si': ['ශ්‍රී ලංකාව ප්‍රජාතන්ත්‍රවාදී රාජ්‍යයකි', 'ශ්‍රී ලංකාව සංවර්ධනය වන රටකි'],
            'fact_en': ['Sri Lanka is a democratic state', 'Sri Lanka is a developing country'],
            'source': ['Constitution', 'World Bank'],
            'url': ['https://www.parliament.lk', 'https://www.worldbank.org']
        })

    def check_facts(self, content: Dict) -> List[Dict]:
        fact_checks = []
        english_facts = super().check_facts(content['english_text'])
        fact_checks.extend(english_facts)

        if content['language'] == 'si':
            sinhala_facts = self._check_sinhala_facts(content['original_text'])
            fact_checks.extend(sinhala_facts)

        return fact_checks

    def _check_sinhala_facts(self, text: str) -> List[Dict]:
        fact_checks = []
        for _, fact in self.sri_lankan_facts.iterrows():
            if self._check_contradiction_sinhala(text, fact['fact_si']):
                fact_checks.append({
                    'claim': text,
                    'correction_si': fact['fact_si'],
                    'correction_en': fact['fact_en'],
                    'source': fact['source'],
                    'source_url': fact['url']
                })
        return fact_checks

    def _check_contradiction_sinhala(self, claim: str, fact: str) -> bool:
        return False

class EnhancedFactCheckingApp:
    def __init__(self):
        self.content_extractor = ContentExtractor()
        self.sri_lankan_extractor = SriLankanNewsExtractor()
        self.fact_checker = EnhancedFactChecker()

    def process_url(self, url: str) -> Dict:
        try:
            if any(domain in url for domain in self.sri_lankan_extractor.known_sources.keys()):
                content = self.sri_lankan_extractor.extract_content(url)
                fact_check_results = self.fact_checker.check_facts(content)

                return {
                    'url': url,
                    'content_type': 'sinhala_news',
                    'original_text': content['original_text'],
                    'english_translation': content['english_text'],
                    'fact_checks': fact_check_results,
                    'status': 'success'
                }

            content_type, content = self.content_extractor.extract_from_url(url)
            fact_check_results = self.fact_checker.check_facts({
                'english_text': content,
                'original_text': content,
                'language': 'en'
            })

            return {
                'url': url,
                'content_type': content_type,
                'fact_checks': fact_check_results,
                'status': 'success'
            }
        except Exception as e:
            return {
                'url': url,
                'status': 'error',
                'error': str(e)
            }

def setup_environment():
    try:
        nltk.download('punkt', quiet=True)
        try:
            spacy.load('en_core_web_sm')
        except OSError:
            os.system('python -m spacy download en_core_web_sm')
        os.system('pip install googletrans==3.1.0a0')

        try:
            import torch
        except ImportError:
            os.system('pip install torch')

        print("Environment setup completed successfully.")
        return True
    except Exception as e:
        print(f"Error setting up environment: {str(e)}")
        return False

if __name__ == "__main__":
    if setup_environment():
        try:
            app = EnhancedFactCheckingApp()

            # Example URL
            url = "https://www.adaderana.lk/news/103288/imf-delegation-to-visit-sri-lanka-next-week"
            results = app.process_url(url)

            if results['status'] == 'success':
                print(f"Content type: {results['content_type']}")
                if results['content_type'] == 'sinhala_news':
                    print("\nOriginal Sinhala text:")
                    print(results['original_text'][:200] + "...")
                    print("\nEnglish translation:")
                    print(results['english_translation'][:200] + "...")

                if results['fact_checks']:
                    print("\nFact check results:")
                    for check in results['fact_checks']:
                        print("\nClaim:")
                        if 'correction_si' in check:
                            print(f"Sinhala: {check['correction_si']}")
                            print(f"English: {check['correction_en']}")
                        else:
                            print(check['claim'])
                            print(f"Correction: {check['correction']}")
                        print(f"Source: {check['source']}")
                        print(f"Source URL: {check['source_url']}")
                else:
                    print("\nNo factual inaccuracies found.")
            else:
                print(f"Error: {results['error']}")

        except Exception as e:
            print(f"Application error: {str(e)}")
    else:
        print("Failed to set up environment. Please check the error messages above.")

pip install googletrans==3.1.0a0 transformers sacremoses sentencepiece

!pip install --upgrade transformers